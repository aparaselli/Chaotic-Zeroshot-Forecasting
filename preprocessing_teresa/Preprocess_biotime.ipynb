{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "Typical Columns\n",
    "\n",
    "STUDY_ID (ID of the study)\n",
    "\n",
    "GENUS_SPECIES (or a similar species identifier)\n",
    "\n",
    "YEAR, MONTH, DAY (often incomplete; sometimes only year is provided)\n",
    "\n",
    "LATITUDE, LONGITUDE\n",
    "\n",
    "ABUNDANCE (or BIOMASS, DENSITY, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>STUDY_ID</th>\n",
       "      <th>DAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>SAMPLE_DESC</th>\n",
       "      <th>PLOT</th>\n",
       "      <th>ID_SPECIES</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>sum.allrawdata.ABUNDANCE</th>\n",
       "      <th>sum.allrawdata.BIOMASS</th>\n",
       "      <th>GENUS</th>\n",
       "      <th>SPECIES</th>\n",
       "      <th>GENUS_SPECIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984</td>\n",
       "      <td>47.400000_-95.120000_12_Control_0_Medium</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>47.40000</td>\n",
       "      <td>-95.12000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Acer</td>\n",
       "      <td>rubrum</td>\n",
       "      <td>Acer rubrum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984</td>\n",
       "      <td>47.400000_-95.120000_12_Control_0_Medium</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>47.40000</td>\n",
       "      <td>-95.12000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Acer</td>\n",
       "      <td>saccharum</td>\n",
       "      <td>Acer saccharum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984</td>\n",
       "      <td>47.400000_-95.120000_12_Control_0_Medium</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>47.40000</td>\n",
       "      <td>-95.12000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Acer</td>\n",
       "      <td>spicatum</td>\n",
       "      <td>Acer spicatum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984</td>\n",
       "      <td>47.400000_-95.120000_12_Control_0_Medium</td>\n",
       "      <td>12</td>\n",
       "      <td>607</td>\n",
       "      <td>47.40000</td>\n",
       "      <td>-95.12000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Corylus</td>\n",
       "      <td>cornuta</td>\n",
       "      <td>Corylus cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984</td>\n",
       "      <td>47.400000_-95.120000_12_Control_0_Small</td>\n",
       "      <td>12</td>\n",
       "      <td>1911</td>\n",
       "      <td>47.40000</td>\n",
       "      <td>-95.12000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Populus</td>\n",
       "      <td>pinnata</td>\n",
       "      <td>Populus pinnata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8552244</th>\n",
       "      <td>26178100</td>\n",
       "      <td>548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "      <td>49.1014548954342_13.3200349605548_T3_56_2007</td>\n",
       "      <td>T3_56</td>\n",
       "      <td>49340</td>\n",
       "      <td>49.10146</td>\n",
       "      <td>13.32004</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vaccinium</td>\n",
       "      <td>vitis.idaea</td>\n",
       "      <td>Vaccinium vitis.idaea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8552245</th>\n",
       "      <td>26179100</td>\n",
       "      <td>548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "      <td>49.1014548954342_13.3200349605548_T3_56_2009</td>\n",
       "      <td>T3_56</td>\n",
       "      <td>49340</td>\n",
       "      <td>49.10146</td>\n",
       "      <td>13.32004</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vaccinium</td>\n",
       "      <td>vitis.idaea</td>\n",
       "      <td>Vaccinium vitis.idaea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8552246</th>\n",
       "      <td>26180100</td>\n",
       "      <td>548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>49.1014548954342_13.3200349605548_T3_56_2012</td>\n",
       "      <td>T3_56</td>\n",
       "      <td>49340</td>\n",
       "      <td>49.10146</td>\n",
       "      <td>13.32004</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vaccinium</td>\n",
       "      <td>vitis.idaea</td>\n",
       "      <td>Vaccinium vitis.idaea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8552247</th>\n",
       "      <td>26181100</td>\n",
       "      <td>548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "      <td>49.097317976565_13.3173542074378_T3_51_2007</td>\n",
       "      <td>T3_51</td>\n",
       "      <td>40355</td>\n",
       "      <td>49.09732</td>\n",
       "      <td>13.31735</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Veronica</td>\n",
       "      <td>chamaedrys</td>\n",
       "      <td>Veronica chamaedrys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8552248</th>\n",
       "      <td>26182100</td>\n",
       "      <td>548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "      <td>49.1014548954342_13.3200349605548_T3_56_2011</td>\n",
       "      <td>T3_56</td>\n",
       "      <td>40355</td>\n",
       "      <td>49.10146</td>\n",
       "      <td>13.32004</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Veronica</td>\n",
       "      <td>chamaedrys</td>\n",
       "      <td>Veronica chamaedrys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8552249 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  STUDY_ID  DAY  MONTH  YEAR  \\\n",
       "0                 1        10  NaN    NaN  1984   \n",
       "1                 2        10  NaN    NaN  1984   \n",
       "2                 3        10  NaN    NaN  1984   \n",
       "3                 4        10  NaN    NaN  1984   \n",
       "4                 5        10  NaN    NaN  1984   \n",
       "...             ...       ...  ...    ...   ...   \n",
       "8552244    26178100       548  NaN    NaN  2007   \n",
       "8552245    26179100       548  NaN    NaN  2009   \n",
       "8552246    26180100       548  NaN    NaN  2012   \n",
       "8552247    26181100       548  NaN    NaN  2007   \n",
       "8552248    26182100       548  NaN    NaN  2011   \n",
       "\n",
       "                                          SAMPLE_DESC   PLOT  ID_SPECIES  \\\n",
       "0            47.400000_-95.120000_12_Control_0_Medium     12          22   \n",
       "1            47.400000_-95.120000_12_Control_0_Medium     12          23   \n",
       "2            47.400000_-95.120000_12_Control_0_Medium     12          24   \n",
       "3            47.400000_-95.120000_12_Control_0_Medium     12         607   \n",
       "4             47.400000_-95.120000_12_Control_0_Small     12        1911   \n",
       "...                                               ...    ...         ...   \n",
       "8552244  49.1014548954342_13.3200349605548_T3_56_2007  T3_56       49340   \n",
       "8552245  49.1014548954342_13.3200349605548_T3_56_2009  T3_56       49340   \n",
       "8552246  49.1014548954342_13.3200349605548_T3_56_2012  T3_56       49340   \n",
       "8552247   49.097317976565_13.3173542074378_T3_51_2007  T3_51       40355   \n",
       "8552248  49.1014548954342_13.3200349605548_T3_56_2011  T3_56       40355   \n",
       "\n",
       "         LATITUDE  LONGITUDE  sum.allrawdata.ABUNDANCE  \\\n",
       "0        47.40000  -95.12000                       1.0   \n",
       "1        47.40000  -95.12000                       3.0   \n",
       "2        47.40000  -95.12000                       1.0   \n",
       "3        47.40000  -95.12000                      12.0   \n",
       "4        47.40000  -95.12000                       1.0   \n",
       "...           ...        ...                       ...   \n",
       "8552244  49.10146   13.32004                       3.0   \n",
       "8552245  49.10146   13.32004                       4.0   \n",
       "8552246  49.10146   13.32004                       3.0   \n",
       "8552247  49.09732   13.31735                      10.0   \n",
       "8552248  49.10146   13.32004                       4.0   \n",
       "\n",
       "         sum.allrawdata.BIOMASS      GENUS      SPECIES          GENUS_SPECIES  \n",
       "0                           0.0       Acer       rubrum            Acer rubrum  \n",
       "1                           0.0       Acer    saccharum         Acer saccharum  \n",
       "2                           0.0       Acer     spicatum          Acer spicatum  \n",
       "3                           0.0    Corylus      cornuta        Corylus cornuta  \n",
       "4                           0.0    Populus      pinnata        Populus pinnata  \n",
       "...                         ...        ...          ...                    ...  \n",
       "8552244                     NaN  Vaccinium  vitis.idaea  Vaccinium vitis.idaea  \n",
       "8552245                     NaN  Vaccinium  vitis.idaea  Vaccinium vitis.idaea  \n",
       "8552246                     NaN  Vaccinium  vitis.idaea  Vaccinium vitis.idaea  \n",
       "8552247                     NaN   Veronica   chamaedrys    Veronica chamaedrys  \n",
       "8552248                     NaN   Veronica   chamaedrys    Veronica chamaedrys  \n",
       "\n",
       "[8552249 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/BioTIMEQuery_24_06_2021.csv', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'STUDY_ID', 'DAY', 'MONTH', 'YEAR', 'SAMPLE_DESC', 'PLOT',\n",
      "       'ID_SPECIES', 'LATITUDE', 'LONGITUDE', 'sum.allrawdata.ABUNDANCE',\n",
      "       'sum.allrawdata.BIOMASS', 'GENUS', 'SPECIES', 'GENUS_SPECIES'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = df.replace(['missing', 'NA', -9999], np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Dates into Tokens\n",
    "Since our time series data comes with date information (e.g., from BioTIME), we first need to encode the dates as tokens. One approach is to:\n",
    "\n",
    "Standardize Date Strings: Convert dates to a standard format such as YYYY-MM-DD.\n",
    "Tokenize the Components: Optionally split the string into tokens (year, month, day) or simply use the full string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/var/folders/mk/2tt5852n0v71d0fbybcjfblh0000gn/T/ipykernel_59325/1240879540.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DAY'].fillna(1, inplace=True)\n",
      "/var/folders/mk/2tt5852n0v71d0fbybcjfblh0000gn/T/ipykernel_59325/1240879540.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MONTH'].fillna(1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            YEAR  MONTH  DAY   DateToken\n",
      "Date                                    \n",
      "1984-01-01  1984    1.0  1.0  1984-01-01\n",
      "1984-01-01  1984    1.0  1.0  1984-01-01\n",
      "1984-01-01  1984    1.0  1.0  1984-01-01\n",
      "1984-01-01  1984    1.0  1.0  1984-01-01\n",
      "1984-01-01  1984    1.0  1.0  1984-01-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# ----- Part 1: Preprocessing Dates and Creating Date Tokens -----\n",
    "\n",
    "# Assume df is your DataFrame with separate YEAR, MONTH, and DAY columns.\n",
    "# Convert these columns to numeric (coercing errors to NaN)\n",
    "df[['DAY', 'MONTH', 'YEAR']] = df[['DAY', 'MONTH', 'YEAR']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill missing DAY and MONTH with default value 1\n",
    "df['DAY'].fillna(1, inplace=True)\n",
    "df['MONTH'].fillna(1, inplace=True)\n",
    "\n",
    "# Drop rows where YEAR is missing, since YEAR is essential\n",
    "df = df.dropna(subset=['YEAR'])\n",
    "\n",
    "# Create a standardized Date column using YEAR, MONTH, and DAY\n",
    "df['Date'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
    "\n",
    "# Drop any rows where the date conversion failed (i.e. NaT values)\n",
    "df = df.dropna(subset=['Date'])\n",
    "\n",
    "# Set the new Date column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Rename 'sum.allrawdata.ABUNDANCE' for easier access\n",
    "df.rename(columns={'sum.allrawdata.ABUNDANCE': 'ABUNDANCE'}, inplace=True)\n",
    "\n",
    "# Use the index (Date) to create a string token in the format \"YYYY-MM-DD\"\n",
    "df['DateToken'] = df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "# ----- (Optional) Save or inspect the DataFrame -----\n",
    "# For example, print the first few rows to verify\n",
    "print(df[['YEAR', 'MONTH', 'DAY', 'DateToken']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a T5 Transformer for Time Series Prediction\n",
    "\n",
    "The Chronos-Bolt models (e.g., Chronos-Bolt-Mini or Chronos-Bolt-Small) are based on the T5 architecture. \n",
    "They treat time series forecasting as a sequence-to-sequence problem where both the input and output are tokenized strings.\n",
    "\n",
    "Input Formatting Example:\n",
    "You can format the input as:\n",
    "\n",
    "print(forecast: <Series_name> <tokenized_date_1> <value_1> ... <tokenized_date_n> <value_n>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tteresattian/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install --upgrade transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSError while loading model: Can't load tokenizer for 'amazon/chronos-bolt-small'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'amazon/chronos-bolt-small' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.\n",
      "If you have a local directory named 'amazon/chronos-bolt-small', please rename or remove it.\n",
      "Trying alternative model: amazon/chronos-bolt-mini\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'amazon/chronos-bolt-mini'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'amazon/chronos-bolt-mini' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2036\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2041\u001b[0m     )\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'amazon/chronos-bolt-small'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'amazon/chronos-bolt-small' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     alternative_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon/chronos-bolt-mini\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying alternative model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malternative_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43malternative_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(alternative_model_name)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Build an input string from a time series.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2036\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2041\u001b[0m     )\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'amazon/chronos-bolt-mini'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'amazon/chronos-bolt-mini' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "# Specify the model name. If 'amazon/chronos-bolt-small' fails, try an alternative.\n",
    "model_name = 'amazon/chronos-bolt-small'\n",
    "try:\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "except OSError as e:\n",
    "    print(\"OSError while loading model:\", e)\n",
    "    print(\"If you have a local directory named 'amazon/chronos-bolt-small', please rename or remove it.\")\n",
    "    # Try an alternative model identifier (if available)\n",
    "    alternative_model_name = 'amazon/chronos-bolt-mini'\n",
    "    print(f\"Trying alternative model: {alternative_model_name}\")\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(alternative_model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(alternative_model_name)\n",
    "\n",
    "# Build an input string from a time series.\n",
    "series_name = \"Oneida_Lake_NY\"\n",
    "time_series_tokens = [\"2020-01-31\", \"5.3\", \"2020-02-29\", \"5.6\", \"2020-03-31\", \"5.8\"]\n",
    "input_text = f\"forecast: {series_name} \" + \" \".join(time_series_tokens)\n",
    "print(\"Input text:\", input_text)\n",
    "\n",
    "# Tokenize the input and generate forecast\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "outputs = model.generate(input_ids, max_length=50)\n",
    "forecast = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Forecasted Output:\", forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "- Chaotic Chrono Paper: arXiv:2409.15771\n",
    "- Chronos-Bolt Model on Hugging Face: amazon/chronos-bolt-small\n",
    "- Chaotic Synthetic Dataset: dysts_data GitHub\n",
    "- Chronos GitHub Repository: amazon-science/chronos-forecasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
